---
title: "Branch ML"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message=FALSE,cache=TRUE)
```
The data used in this model is the exact same as what has been used in the actual model. I have just anonymised the preditor variables and target variable. The target variable is simply a 1 (where monthly score exceeds threshold) and 0 when the score doesn't exceed the threshold.

```{r results='asis'}

library(plyr)
library(caret)
library(lmtest)
library(pscl)
library(MKmisc)
library(survey)
library(InformationValue)
library(tidyverse)
library(corrplot)
library(MASS)
library(ROCR)
library(corrr)
library(tidyquant)
```


```{r results='asis'}
#read data into R, mutate target variable as factor
data_18122018v3_anon<-read_csv("C:/Users/Michael/Desktop/data_18122018v3_anon.csv")
data<-data_18122018v3_anon%>%dplyr::mutate(target_var=as.factor(target_var))
```

```{r}
summary(data)
```
           

There are no N/As in the data, so we're right to go.
Lets check the correlations and importance of variables.

```{r }
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=10)
# train the model
model_vi <- train(target_var~., data=data, method="pls", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model_vi, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)
```


The model suggests that var7, var9, var12 and var13 are important variables to be considered when building the final model 

```{r }
corrr_analysis <- data %>% mutate(target_var=as.numeric(target_var))%>%
  correlate() %>%
  focus(target_var) %>%
  #rename(feature == rowname) %>%
  arrange(abs(target_var)) %>%
  mutate(feature = as_factor(rowname)) 
corrr_analysis

corrr_analysis %>%
  ggplot(aes(x = target_var, y = fct_reorder(feature, desc(target_var)))) +
  geom_point() +
  # Positive Correlations - Contribute to risk
  geom_segment(aes(xend = 0, yend = feature), 
               color = palette_light()[[3]], 
               data = corrr_analysis %>% filter(target_var >0)) +
  geom_point(color = palette_light()[[3]], 
             data = corrr_analysis %>% filter(target_var >0 )) +
  
  # Negative Correlations - Prevent risk
    geom_segment(aes(xend = 0, yend = feature), 
                 color = palette_light()[[2]], 
                 data = corrr_analysis %>% filter(target_var<0 )) +
    geom_point(color = palette_light()[[2]], 
               data = corrr_analysis %>% filter(target_var<0 )) +
# Vertical lines
  geom_vline(xintercept = 0, color = palette_light()[[4]], size = 1, linetype = 2) +
  geom_vline(xintercept = -0.25, color = palette_light()[[4]], size = 1, linetype = 2) +
  geom_vline(xintercept = 0.25, color = palette_light()[[4]], size = 1, linetype = 2) +
  # Aesthetics
  theme_tq() +
  labs(title = "Branch Risk Indicator Correlation Analysis",
       subtitle = paste("Positive Correlations (contribute to risk),",
                        "Negative Correlations (reduce risk)"),
       y = "Feature Importance",
       x= "Target")

```

Checking for linear combiantions and highlight variables with near zero variance
```{r }

#find linear combinations
findLinearCombos(data)

#find near zero variance

nearZeroVar(data,names=TRUE,saveMetrics = TRUE)


```



Now I'll use the Caret package to train/fit GLM model predictor as factor

First I'll set the control parameters, then split the data into a training and test set. Because I am also looking at performing 10-fold cross validation on the data, I won't set aside a validation set.

```{r }
control <- trainControl(method="repeatedcv", number=10, repeats=10)

Train <- createDataPartition(data$target_var, p=0.7, list=FALSE)
training <- data[ Train, ]
testing <- data [ -Train, ]
```

```{r eval=FALSE}
##this finds the best combination of variables for logit##
mod_fit<-train(target_var~.,
               data=training, method="glmStepAIC", family="binomial",trControl = control)

mod_fit$results
mod_fit$finalModel
summary(mod_fit$finalModel)
```

While the model fit suggests an equation with quite a high accuracy, in practice the list of actual variables makes it very difficult for the goals of this project to be met i.e. develop an algorithm that can be used on a daily basis using variables I can extract easily, without relying on another department to deliver the data. 
Based on the above criteria, I chose to two specific variables that I knew could be obtained and loaded into the Qliksense app daily. Each month I run the whole algorithm again with all the variables to ensure that my manual selection is still valid.

```{r warning=FALSE}

#select variables manually

mod_fit_one<-train(target_var~var8+var9,
                   data=training, method="glm", family="binomial",trControl = control)

```

The results of the equation are below

```{r }
summary(mod_fit_one)

pred <- predict.train(mod_fit_one, newdata=testing)

results<-table(pred, testing$target_var)

results

sum(diag(results))/sum(results)

```

The model is reasonably accurate, with a higher proportion of false positives than false negatives. For the purposes of this model, that is acceptable.

#Conclusion

While this model is not the best performing (Naive Bayes and Neural Net models performed better) and is not even the suggested equation from Caret, it achieves exactly what I was trying to achieve.
